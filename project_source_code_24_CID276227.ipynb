{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phon1e/political_bias_on_fake_news/blob/main/project_source_code_24_CID276227.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Impact of Political Bias on Fake News\n",
        "Detection and Bias Mitigation\" project source code\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YTN3VJaKKg5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATA COLLECTION"
      ],
      "metadata": {
        "id": "NJ43eiKd_JXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "C4CNqVRT_eE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## DATASET (LIAR + Buzzfeed)"
      ],
      "metadata": {
        "id": "2PMWR8M3_YZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LIAR dataset\n",
        "def read_dataframe(tsv_file:str)->pd.DataFrame:\n",
        "    \"\"\"read lair dataset and convert to dataframe\n",
        "\n",
        "    Args:\n",
        "        tsv_file (string): tsv file path\n",
        "\n",
        "    Returns:\n",
        "        dataframe: dataframe of the lair dataset\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(tsv_file, delimiter='\\t', dtype=object)\n",
        "    df.fillna(\"\", inplace=True)\n",
        "    df.columns = [\n",
        "        'id',                # Column 1: the ID of the statement ([ID].json).\n",
        "        'label',             # Column 2: the label.\n",
        "        'statement',         # Column 3: the statement.\n",
        "        'subjects',          # Column 4: the subject(s).\n",
        "        'speaker',           # Column 5: the speaker.\n",
        "        'speaker_job_title', # Column 6: the speaker's job title.\n",
        "        'state_info',        # Column 7: the state info.\n",
        "        'party_affiliation', # Column 8: the party affiliation.\n",
        "\n",
        "        # Column 9-13: the total credit history count, including the current statement.\n",
        "        'count_1', # barely true counts.\n",
        "        'count_2', # false counts.\n",
        "        'count_3', # half true counts.\n",
        "        'count_4', # mostly true counts.\n",
        "        'count_5', # pants on fire counts.\n",
        "\n",
        "        'context' # Column 14: the context (venue / location of the speech or statement).\n",
        "    ]\n",
        "    return df\n",
        "\n",
        "\n",
        "#Buzzfeed dataset\n",
        "def xml_to_df(f_path:str)->pd.DataFrame:\n",
        "    \"\"\"read all xml files in the folder and convert to dataframe by tag\n",
        "    <mainText>, <orientation>, <title>, <veracity>\n",
        "\n",
        "    Args:\n",
        "        f_path (string): folder path\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: dataframe of the xml files\n",
        "    \"\"\"\n",
        "    all_files = os.listdir(f_path)\n",
        "    data = []\n",
        "    for file in all_files:\n",
        "        tree = ET.parse(f_path + '\\\\' + file)\n",
        "        root = tree.getroot()\n",
        "        main_text = root.find('mainText').text\n",
        "        orientation = root.find('orientation').text\n",
        "        title = root.find('title').text\n",
        "        veracity = root.find('veracity').text\n",
        "        data.append([title, main_text, orientation, veracity])\n",
        "    df = pd.DataFrame(data, columns=['title', 'content', 'bias', 'label'])\n",
        "    return df"
      ],
      "metadata": {
        "id": "lQ8l3yga_aEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = 'liar_ds/'\n",
        "liar_df = read_dataframe(f'{fp}/train.tsv')"
      ],
      "metadata": {
        "id": "sFc5Jxwc_vOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_path = \"\\articles\"\n",
        "buzzfeed_bias_df = xml_df = xml_to_df(f_path)"
      ],
      "metadata": {
        "id": "cHo04eSnAdQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "bxR6YfCNAa3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_bar_chart(input_df: pd.DataFrame, title: str = \"LIAR Dataset\") -> None:\n",
        "    \"\"\"plot lair dataset as bar chart\n",
        "\n",
        "    Args:\n",
        "        input_df (pd.DataFrame): _description_\n",
        "        title (str, optional): _description_. Defaults to \"LIAR Dataset\".\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    # computes frequencies of labels and converts to percentages\n",
        "    label_frequencies = input_df['label'].value_counts(normalize=True)\n",
        "\n",
        "    def multiply_100(x):\n",
        "        return x * 100\n",
        "\n",
        "    # \"apply\" is a handy way to call a function on every row of data.\n",
        "    label_frequencies = label_frequencies.apply(multiply_100)\n",
        "\n",
        "    # bar chart ordering and  colors for readability.\n",
        "    labels = ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']\n",
        "    colors = [\n",
        "        'orangered', # pants-fire\n",
        "        'coral', # false\n",
        "        'salmon', # barely-true\n",
        "        'peachpuff', # half-true\n",
        "        'skyblue', # mostly-true\n",
        "        'deepskyblue' # true\n",
        "    ]\n",
        "\n",
        "    label_frequencies = label_frequencies.reindex(index = labels)\n",
        "\n",
        "\n",
        "    # creates a horizontal bar chart with a descriptive title\n",
        "    axis = label_frequencies.plot(kind='barh', figsize=(4, 2), color=colors)\n",
        "    axis.set_title(f\"distribution of label values ({title}, sample_size={len(input_df)})\", size=20);"
      ],
      "metadata": {
        "id": "DrM1AnYIAUM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mapping label between two dataset\n",
        "\n",
        "label_mapping = {\n",
        "    \"pants-fire\": \"false\",\n",
        "    \"false\": \"false\",\n",
        "    \"barely true\": \"mixed\",\n",
        "    \"half-true\": \"mixed\",\n",
        "    \"mostly-true\": \"true\",\n",
        "    \"true\": \"true\"\n",
        "}\n",
        "\n",
        "news_liar_bias_df = liar_df.copy()\n",
        "news_liar_bias_df['bias'] = \"unk\"\n",
        "news_liar_bias_df['harmonized_label'] = news_liar_bias_df['label'].map(label_mapping)\n",
        "\n",
        "label_mapping_bf = {\n",
        "    \"mostly true\": \"true\",\n",
        "    \"mixture of true and false\": \"mixed\",\n",
        "    \"mostly false\": \"false\",\n",
        "    \"no factual content\": \"false\"\n",
        "}\n",
        "\n",
        "buzzfeed_bias_df['harmonized_label'] = buzzfeed_bias_df['label'].map(label_mapping_bf)\n",
        "\n",
        "#rename buzzfeed_bias_df columns content to statement\n",
        "buzzfeed_bias_df.rename(columns={'content': 'statement'}, inplace=True)\n",
        "\n",
        "buzzfeed_bias_df.harmonized_label.value_counts()"
      ],
      "metadata": {
        "id": "EICq_qgsCHjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#handle missing value\n",
        "buzzfeed_bias_df = buzzfeed_bias_df.dropna()\n",
        "\n",
        "#combine liar and buzzfeed dataset statement, harmonized_label and bias\n",
        "news_liar_bias_df = news_liar_bias_df[['statement', 'harmonized_label', 'bias']]\n",
        "buzzfeed_bias_df = buzzfeed_bias_df[['statement', 'harmonized_label', 'bias']]\n",
        "\n",
        "#combine liar and buzzfeed dataset\n",
        "combined_df = pd.concat([news_liar_bias_df, buzzfeed_bias_df])\n",
        "#drop na\n",
        "combine_df = combined_df.dropna()"
      ],
      "metadata": {
        "id": "QIoLSaiiCSKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING AND CLASSIFIER"
      ],
      "metadata": {
        "id": "ChKk4SHkFkkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words classifier"
      ],
      "metadata": {
        "id": "fM2RB5bEC8Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "#bias clf 0,1,2 mainstream, left, right\n",
        "\n",
        "#bias clf\n",
        "def bow_clf(df, col1, col2):\n",
        "   \"\"\"classifier using bag of words\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): _description_\n",
        "        col1 (str): column target1\n",
        "        col2 (str): column target2\n",
        "\n",
        "    Returns:\n",
        "        classifer and count vectorizer\n",
        "    \"\"\"\n",
        "    #split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split( df[col1], df[col2], test_size=0.2, random_state=42)\n",
        "\n",
        "    #count vectorizer\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    X_train_counts = count_vectorizer.fit_transform(X_train)\n",
        "    X_test_counts = count_vectorizer.transform(X_test)\n",
        "\n",
        "    #naive bayes\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X_train_counts, y_train)\n",
        "\n",
        "    #predict\n",
        "    pred = clf.predict(X_test_counts)\n",
        "\n",
        "    #classification report\n",
        "    print(classification_report(y_test, pred))\n",
        "    return clf, count_vectorizer"
      ],
      "metadata": {
        "id": "vvQp_eEfCvly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and evaluate function"
      ],
      "metadata": {
        "id": "f6q-JJGzC9UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def train_and_evaluate(model,tokenizer, df, col1,col2, savename):\n",
        "   \"\"\"train and evaluate model\n",
        "\n",
        "    Args:\n",
        "        model : transformer-based model\n",
        "        tokenizer : transformer-based tokenizer\n",
        "        df: dataframe\n",
        "        col1: column target1\n",
        "        col2: column target2\n",
        "        savename: name of the saved model\n",
        "    Returns:\n",
        "        model and tokenizer\n",
        "\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(device)\n",
        "\n",
        "    # Ensure the correct columns are used\n",
        "    df = df[[col1, col2]]\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(df[col1].tolist(), df[col2].tolist(), test_size=0.2, random_state=42)\n",
        "\n",
        "    # Tokenize the data\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "    # Create a PyTorch Dataset\n",
        "    class BiasDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, encodings, labels):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "            item['labels'] = torch.tensor(self.labels[idx])\n",
        "            return item\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "    train_dataset = BiasDataset(train_encodings, train_labels)\n",
        "    val_dataset = BiasDataset(val_encodings, val_labels)\n",
        "\n",
        "    # Load the model and set up training\n",
        "    model = model\n",
        "    model.to(device)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    def compute_metrics(p):\n",
        "        preds = p.predictions.argmax(-1)\n",
        "        labels = p.label_ids\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "        return {\n",
        "            'accuracy': acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    #save the model\n",
        "    model.save_pretrained(f'saved_model\\\\{savename}')\n",
        "    tokenizer.save_pretrained(f'saved_model\\\\{savename}')\n",
        "\n",
        "    # Evaluate the model\n",
        "    results = trainer.evaluate()\n",
        "\n",
        "    # Function to make predictions\n",
        "    def predict(texts):\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encodings)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        return predictions.tolist()\n",
        "\n",
        "    return results, predict"
      ],
      "metadata": {
        "id": "j1s6OgyuDGxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL"
      ],
      "metadata": {
        "id": "-fukE551IYee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Roberta model"
      ],
      "metadata": {
        "id": "1dlFnb8fHWWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ids_mask(sentences):\n",
        "     \"\"\"\n",
        "    Get input ids and attention masks for a list of sentences.\n",
        "    Args:\n",
        "        sentences (list): List of sentences.\n",
        "    Returns:\n",
        "        roberta_input_ids (torch.Tensor): Tensor of input ids.\n",
        "        roberta_attention_masks (torch.Tensor): Tensor of attention masks.\n",
        "        label (torch.Tensor): Tensor of labels.\n",
        "        sent_ids (torch.Tensor): Tensor of sentence ids.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    max_len = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        input_ids = roberta_tokenizer.encode(sent, add_special_tokens=True)\n",
        "        max_len = max(max_len, len(input_ids))\n",
        "\n",
        "    roberta_input_ids = []\n",
        "    roberta_attention_masks = []\n",
        "    sents_ids = []\n",
        "    counter = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        roberta_encoded_dict = roberta_tokenizer.encode_plus(\n",
        "            sent,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=64,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        roberta_input_ids.append(roberta_encoded_dict['input_ids'])\n",
        "        roberta_attention_masks.append(roberta_encoded_dict['attention_mask'])\n",
        "\n",
        "        #collect ids\n",
        "        sents_ids.append(counter)\n",
        "        counter += 1\n",
        "\n",
        "    #cvt to tensor\n",
        "    roberta_input_ids = torch.cat(roberta_input_ids, dim=0)\n",
        "    roberta_attention_masks = torch.cat(roberta_attention_masks, dim=0)\n",
        "\n",
        "    label=torch.tensor(labels)\n",
        "    sent_ids = torch.tensor(sents_ids)\n",
        "\n",
        "    return roberta_input_ids, roberta_attention_masks, label, sent_ids\n",
        "\n",
        "roberta_input_ids, roberta_attention_masks, label, sent_ids = get_ids_mask(sentences)"
      ],
      "metadata": {
        "id": "EJ28pXiXHYA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DistilBERT model and Bag of Words model (political bias)"
      ],
      "metadata": {
        "id": "BfsHmpw3DPGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results_bias_clf_distilbert, predict_distilbert = train_and_evaluate(\n",
        "    AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3),\n",
        "    AutoTokenizer.from_pretrained('distilbert-base-uncased'),\n",
        "    bias_clf_df,\n",
        "    'statement',\n",
        "    'bias',\n",
        "    savename='bias_distil_model_2'\n",
        ")\n",
        "\n",
        "clf_fake_news_df, vectorizer_bow = bow_clf(combined_bias_fact_df, 'statement', 'bias')"
      ],
      "metadata": {
        "id": "PRzbF09uDRs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DistilBERT model and Bag of Words model (fake news)"
      ],
      "metadata": {
        "id": "e5ndcPwzDKxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fake_news = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels= num_labels)\n",
        "tkn = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "results_fakeN_clf_distilbert, predict_distilbert_FN = train_and_evaluate(\n",
        "    AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2),\n",
        "    AutoTokenizer.from_pretrained('distilbert-base-uncased'),\n",
        "    true_n_false_df,\n",
        "    'statement',\n",
        "    'fake_news',\n",
        "    savename='bias_distil_model_2'\n",
        ")\n",
        "\n",
        "clf_fakeN_df, vectorizer_FN = bow_clf(combined_bias_fact_df, 'statement', 'fake_news')"
      ],
      "metadata": {
        "id": "2pHvcFVOD0gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MASKING MODEL (bias classifier with attention score)"
      ],
      "metadata": {
        "id": "U3mIoXG9IA0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def detect_and_mask(sentence, model, tokenizer, nlp, top_k=0.2):\n",
        "    \"\"\"\n",
        "    Detect and mask named entities in a sentence based on attention scores.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): Input sentence.\n",
        "        model (torch.nn.Module): Language model.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer.\n",
        "        nlp (spacy.lang.en.English): spaCy English language model.\n",
        "        top_k (float): Proportion of tokens to mask.\n",
        "\n",
        "    Returns:\n",
        "        str: Masked sentence.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    additional_avoid_words = set(['.', ',', '!', '?', '(', ')', \"'\", '\"', 'is', 'am', 'are', 'was', 'were',\n",
        "                                  'be', 'being', 'been', 'am', 'do', 'does', 'did',\n",
        "                                  'have', 'has', 'had', '', ' '])\n",
        "    avoid_words = stop_words.union(additional_avoid_words)\n",
        "    # Tokenize the input sentence\n",
        "    inputs = tokenizer(sentence, return_tensors='pt',\n",
        "                       truncation=True, padding=True,\n",
        "                        max_length=512)\n",
        "\n",
        "    # Ensure the model outputs attentions\n",
        "    outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    # Extract attention scores\n",
        "    attentions = outputs.attentions[-1]  # Get attention from the last layer\n",
        "\n",
        "    # Average attention scores across heads and tokens\n",
        "    attention_scores = attentions.mean(dim=1).squeeze(0).mean(dim=0)  # Average over heads and batch dimension, keep token dimension\n",
        "\n",
        "    # Get the tokenized input tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\n",
        "\n",
        "    # Use spaCy NER to identify named entities\n",
        "    doc = nlp(sentence)\n",
        "    ner_indices = set()\n",
        "\n",
        "    # Manually map character-level positions to token-level\n",
        "    token_offsets = []\n",
        "    current_char_index = 0\n",
        "    for token in tokens:\n",
        "        if token in ['[CLS]', '[SEP]', '[PAD]', '<mask>']:\n",
        "            token_offsets.append((current_char_index, current_char_index))\n",
        "            continue\n",
        "        current_char_index = sentence.find(token.replace('##', ''), current_char_index)\n",
        "        token_offsets.append((current_char_index, current_char_index + len(token.replace('##', ''))))\n",
        "        current_char_index += len(token.replace('##', ''))\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        for idx, (start, end) in enumerate(token_offsets):\n",
        "            if start >= ent.start_char and end <= ent.end_char:\n",
        "                ner_indices.add(idx)\n",
        "\n",
        "    # Ignore special tokens, stopwords, and additional avoid words\n",
        "    valid_indices = [i for i, token in enumerate(tokens) if token.lower() not in avoid_words and token not in ['[CLS]', '[SEP]', '[PAD]', '<mask>']]\n",
        "\n",
        "    # Ensure NER indices are considered in valid indices\n",
        "    valid_indices = list(set(valid_indices + list(ner_indices)))\n",
        "\n",
        "    # Filter attention scores for valid tokens\n",
        "    filtered_attention_scores = attention_scores[valid_indices]\n",
        "    filtered_tokens = [tokens[i] for i in valid_indices]\n",
        "\n",
        "    # Identify top-k% tokens based on filtered attention scores\n",
        "    num_tokens_to_mask = int(len(filtered_tokens) * top_k)\n",
        "    top_k_indices = torch.topk(filtered_attention_scores, num_tokens_to_mask, largest=True).indices.tolist()\n",
        "\n",
        "    # Create a masked sentence\n",
        "    masked_tokens = tokens.copy()\n",
        "    for idx in top_k_indices:\n",
        "        masked_tokens[valid_indices[idx]] = '<mask>'\n",
        "\n",
        "    masked_sentence = tokenizer.convert_tokens_to_string(masked_tokens)\n",
        "\n",
        "    return masked_sentence"
      ],
      "metadata": {
        "id": "ndMJ7WLnICye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FILL TEXT MODEL (Trained BART)"
      ],
      "metadata": {
        "id": "i_uouXflIc6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import Trainer, TrainingArguments, GenerationConfig\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenize function for the BART model.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): Input examples.\n",
        "\n",
        "    Returns:\n",
        "        dict: Tokenized examples.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(examples[\"masked_statement\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples[\"statement\"], max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "model_name = \"facebook/bart-large\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "dataset = Dataset.from_pandas(df_mainstream)\n",
        "dataset = Dataset.from_pandas(df_mainstream2)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Define generation configuration\n",
        "generation_config = GenerationConfig(\n",
        "    early_stopping=True,\n",
        "    num_beams=4,\n",
        "    no_repeat_ngram_size=3,\n",
        "    forced_bos_token_id=0,\n",
        "    forced_eos_token_id=2\n",
        ")\n",
        "\n",
        "# Save generation configuration\n",
        "generation_config.save_pretrained(\"/bart_model_mainstream\")\n",
        "\n",
        "# Export model and tokenizer\n",
        "model.save_pretrained(\"/bart_model_mainstream\")\n",
        "tokenizer.save_pretrained(\"/bart_tkn_mainstream\")\n"
      ],
      "metadata": {
        "id": "pbZ34ijwIhzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PARAPHRASE MODEL"
      ],
      "metadata": {
        "id": "3y7ob7HDIJza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "model_name = 'tuner007/pegasus_paraphrase'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(torch_device)\n",
        "\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def get_response(input_text,num_return_sequences,num_beams):\n",
        "    \"\"\"\n",
        "    Get a response from model (PEGASUS).\n",
        "\n",
        "    Args:\n",
        "        input_text (str): input text\n",
        "        num_return_sequences (int): number of return sequences\n",
        "        num_beams (int): number of beams\n",
        "\n",
        "    Returns:\n",
        "        str: response from model\n",
        "    \"\"\"\n",
        "  batch = tokenizer([input_text],truncation=True,padding=True,max_length=60, return_tensors=\"pt\")\n",
        "  translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
        "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "  return tgt_text\n",
        "\n",
        "def paraphrase_txtv2(statement):\n",
        "    \"\"\"\n",
        "    Paraphrase a statement using the Pegasus model.\n",
        "\n",
        "    Args:\n",
        "        statement (str): Input statement.\n",
        "\n",
        "    Returns:\n",
        "        str: Paraphrased statement.\n",
        "    \"\"\"\n",
        "    paraphrased = get_response(input_text=statement, num_return_sequences=1, num_beams=1)[0]\n",
        "    sentences = paraphrased.split('. ')\n",
        "    return sentences[0].strip() + '.'"
      ],
      "metadata": {
        "id": "3dNC3DnGILxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REGENERATE FUNCTION (BART)"
      ],
      "metadata": {
        "id": "Z7LmHxxiI5xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_batch(masked_sents, model, tokenizer, grad_=False, num_iterations=3, learning_rate=1e-5, alpha= 0.1):\n",
        "     \"\"\"\n",
        "    Generate text using the BART model.\n",
        "    Args:\n",
        "        masked_sents (list): List of masked sentences.\n",
        "        model (torch.nn.Module): BART model.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        list: List of generated sentences.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "\n",
        "    inputs = tokenizer(masked_sents, return_tensors='pt', max_length=100, truncation=True, padding=True).to(device)\n",
        "\n",
        "    if grad_:\n",
        "        model.train()  # Enable training mode to allow gradient updates\n",
        "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        for _ in range(num_iterations):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Compute neutralization loss\n",
        "            logits = outputs.logits\n",
        "            softmax_logits = F.softmax(logits, dim=-1)\n",
        "            uniform_dist = torch.full_like(softmax_logits, 1.0 / softmax_logits.size(-1))\n",
        "            neutralization_loss = F.kl_div(softmax_logits.log(), uniform_dist, reduction='batchmean')\n",
        "\n",
        "            # Total loss with balancing\n",
        "            total_loss = loss + alpha * neutralization_loss\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # Update model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()  # Disable training mode for generation\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=100, num_beams=4, no_repeat_ngram_size=2, early_stopping=True)\n",
        "\n",
        "    neutral_sents = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return neutral_sents\n",
        "\n"
      ],
      "metadata": {
        "id": "nYAwGsaLI9Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION"
      ],
      "metadata": {
        "id": "I_mDRkbgJ-Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(input_text, model, tokenizer, classes_label, max_length=512):\n",
        "    \"\"\"\n",
        "    Predict the class of the input text using the provided model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): Input text for prediction.\n",
        "        model (torch.nn.Module): Language model.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer.\n",
        "        classes_label (list): List of class labels.\n",
        "        max_length (int): Maximum sequence length.\n",
        "\n",
        "    Returns:\n",
        "        str: Predicted class label.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    pred_class = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    return classes_label[pred_class]"
      ],
      "metadata": {
        "id": "2D67V1NIJ_nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def calculate_bleu(statement, generated_statement):\n",
        "    \"\"\"\n",
        "    Calculate the BLEU score between the original statement and the generated statement.\n",
        "\n",
        "    Args:\n",
        "        statement (str): Original statement.\n",
        "        generated_statement (str): Generated statement.\n",
        "\n",
        "    Returns:\n",
        "        float: BLEU score.\n",
        "\n",
        "    \"\"\"\n",
        "    result = bleu_metric.compute(predictions=[generated_statement], references=[[statement]])\n",
        "    return result['score']"
      ],
      "metadata": {
        "id": "PzESGbKCKbGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXTRA"
      ],
      "metadata": {
        "id": "5Ud7wJANKGas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def find_common_context_words_and_generate_wordclouds(dataframe, text_column, bias_column, bias_types, stop_words, num_common_words=50, max_words_cloud=100):\n",
        "\n",
        "    \"\"\"\n",
        "    Find the most common context-specific words for each bias type and generate word clouds.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pandas.DataFrame): Input DataFrame.\n",
        "        text_column (str): Name of the text column.\n",
        "        bias_column (str): Name of the bias column.\n",
        "        bias_types (list): List of bias types.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing common context-specific words for each bias type.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def tokenize(text):\n",
        "        text = text.lower()  # Convert to lowercase\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text)  # Extract words\n",
        "        return tokens\n",
        "\n",
        "    # Dictionary to store common context-specific words for each bias type\n",
        "    common_words_by_bias = {}\n",
        "\n",
        "    for bias in bias_types:\n",
        "        # Tokenize statements by bias type, filtering out possible null values\n",
        "        bias_tokens = dataframe[dataframe[bias_column] == bias][text_column].dropna().apply(tokenize).sum()\n",
        "\n",
        "        # Filter out all stop words from tokens\n",
        "        context_words_filtered = [word for word in bias_tokens if word not in stop_words]\n",
        "\n",
        "        # Get the most common context-specific words for the current bias type\n",
        "        common_context_words = Counter(context_words_filtered).most_common(num_common_words)\n",
        "\n",
        "        # Store the result in the dictionary\n",
        "        common_words_by_bias[bias] = common_context_words\n",
        "\n",
        "        # Generate word cloud\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=max_words_cloud).generate_from_frequencies(dict(common_context_words))\n",
        "\n",
        "        # Display the word cloud using matplotlib\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Word Cloud for {bias} Bias')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    return common_words_by_bias\n",
        "\n",
        "# Define stop words manually\n",
        "manual_stop_words = set([\n",
        "    'the', 'a', 'to', 'and', 'of', 'in', 'that', 'is', 's','t', 'it', 'for', 'on', 'with', 'as', 'was', 'by', 'he', 'at',\n",
        "    'from', 'his', 'her', 'an', 'be', 'this', 'which', 'or', 'we', 'you', 'but', 'not', 'are', 'i', 'they', 'have',\n",
        "    'has', 'had', 'do', 'does', 'did', 'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'am',\n",
        "    'if', 'then', 'them', 'there', 'their', 'were', 'been', 'so', 'what', 'when', 'where', 'who', 'whom', 'why', 'how', 'cls'\n",
        "])\n",
        "\n",
        "def print_bias_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Print bias metrics including accuracy, precision, recall, and F1 score.\n",
        "\n",
        "    Args:\n",
        "        y_true (list): List of true labels.\n",
        "        y_pred (list): List of predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    y_pred_mapped = ['NaN' if pred == 'center' else pred for pred in y_pred]\n",
        "\n",
        "    # Filter out \"center\" predictions from both true and predicted labels\n",
        "    y_true_filtered = [true for true, pred in zip(y_true, y_pred_mapped) if pred != 'NaN']\n",
        "    y_pred_filtered = [pred for pred in y_pred_mapped if pred != 'NaN']\n",
        "\n",
        "    # Print metrics\n",
        "    print('Accuracy:', accuracy_score(y_true_filtered, y_pred_filtered))\n",
        "    print('Precision:', precision_score(y_true_filtered, y_pred_filtered, average='macro', zero_division=0))\n",
        "    print('Recall:', recall_score(y_true_filtered, y_pred_filtered, average='macro', zero_division=0))\n",
        "    print('F1:', f1_score(y_true_filtered, y_pred_filtered, average='macro', zero_division=0))\n",
        "    print()\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true_filtered, y_pred_filtered, target_names=['left', 'right'], zero_division=0))\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def calculate_bleu_nltk(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate the BLEU score using NLTK.\n",
        "\n",
        "    Args:\n",
        "        reference (str): Reference text.\n",
        "        candidate (str): Candidate text.\n",
        "\n",
        "    Returns:\n",
        "        float: BLEU score.\n",
        "\n",
        "    \"\"\"\n",
        "    reference_tokens = [reference.split()]\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "    # Calculate BLEU score with smoothing function\n",
        "    bleu_score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothing_function) * 100\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def calculate_confidence_scores(model, tokenizer, dataframe, col1, col2):\n",
        "    \"\"\"\n",
        "    Calculate confidence scores for each statement in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Language model.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer.\n",
        "        dataframe (pandas.DataFrame): Input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with confidence scores.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the neutral statements\n",
        "    neutral_statements = dataframe[col1].tolist()\n",
        "    inputs = tokenizer(neutral_statements, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Get predictions and raw logits from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Get the predicted class and confidence score for each statement\n",
        "    predicted_classes = torch.argmax(probabilities, dim=1)\n",
        "    confidence_scores = probabilities.max(dim=1).values\n",
        "\n",
        "    # Map predicted classes to their corresponding labels\n",
        "    class_mapping = {0: 'mainstream', 1: 'left', 2: 'right'}\n",
        "    dataframe['predicted_class'] = predicted_classes.numpy()\n",
        "    dataframe['predicted_class'] = dataframe['predicted_class'].map(class_mapping)\n",
        "\n",
        "    # Add confidence scores to the DataFrame\n",
        "    dataframe['confidence_score'] = confidence_scores.numpy()\n",
        "\n",
        "    # Compare the predicted class with the original pred_gen_bias\n",
        "    dataframe['is_correct'] = dataframe['predicted_class'] == dataframe[col2]\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "#XAI (explainable ai)\n",
        "\n",
        "def predict_proba(texts):\n",
        "    \"\"\"\n",
        "    Predict the probabilities of each class for the given texts (bias).\n",
        "    Args:\n",
        "        texts (list): List of input texts.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: output of attention score.\n",
        "\n",
        "    \"\"\"\n",
        "    inputs = ft_fake_news_tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = ft_fake_news_model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probs = logits.softmax(dim=1).detach().numpy()\n",
        "\n",
        "    return probs\n",
        "\n",
        "def predict_proba_bias(texts):\n",
        "    \"\"\"\n",
        "    Predict the probabilities of each class for the given texts(fake news).\n",
        "    Args:\n",
        "        texts (list): List of input texts.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: output of attention score.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = ft_bias_tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    outputs = ft_bias_model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probs = logits.softmax(dim=1).detach().numpy()\n",
        "\n",
        "    return probs\n"
      ],
      "metadata": {
        "id": "lNqfS6jQKS6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model and dataset: https://1drv.ms/f/c/00c8e7e1fdd53826/EkeR2y6DghlFuTATk1J289EBpXURIzMah_vyGRGAc6vSfA?e=av7mcV\n",
        "\n",
        "-buzzfeed dataset: https://zenodo.org/records/1239675\n",
        "\n",
        "-LIAR dataset : https://paperswithcode.com/dataset/liar"
      ],
      "metadata": {
        "id": "Dk1BIeJXOfVF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NGptxCGPOuc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}